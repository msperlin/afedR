```{r solution, echo = FALSE, results = "hide", cache=TRUE}
my_link <- 'https://www.gutenberg.org/ebooks/2264.txt.utf-8'

my_book <- readr::read_lines(my_link)

# solution
full_text <- paste0(my_book, collapse = '\n')
my_split <- stringr::str_split(full_text, 
                               pattern = stringr::fixed(' '))[[1]]

# remove empty
my_split <- my_split[my_split != '']

my_tab <- sort(table(my_split), decreasing = TRUE)

# solution
my_sol <- names(my_tab[1])
```

```{r datageneration, echo = FALSE, results = "hide"}
my_answers <- make_random_answers(my_sol, 
                                  candidates = names(my_tab[1:10]))

#check_answers(my_answers)
```


Question
========

For the `full_text` object created earlier, use the `stringr::str_split` function to split the entire text using blanks. Based on the resulting vector, create a frequency table. What is the most used word in the text? Tip: Remove all cases of empty characters.

```{r questionlist, echo = FALSE, results = "asis"}
exams::answerlist(my_answers, markup = "markdown")
```

Solution
================

```{r solution, eval=FALSE}
```


Meta-information
================
extype: schoice
exsolution: `r mchoice2string(c(TRUE, FALSE, FALSE, FALSE, FALSE), single = TRUE)`
exname: "pride and prejudice"
exshuffle: TRUE

